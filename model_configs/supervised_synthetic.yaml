fit:
  seed_everything: 7

  trainer:
    enable_checkpointing: true
    callbacks:
      - class_path: pytorch_lightning.callbacks.LearningRateMonitor
        init_args:
          logging_interval: epoch
      - class_path: pytorch_lightning.callbacks.ModelCheckpoint
        init_args:
          save_top_k: 1
          monitor: val_ExpRate
          mode: max
          filename: '{epoch}-{step}-{val_ExpRate:.4f}'
      - class_path: pytorch_lightning.callbacks.ModelCheckpoint
        init_args:
          save_top_k: 1
          monitor: val_loss
          mode: min
          filename: '{epoch}-{step}-{val_loss:.4f}'
#    accelerator: 'cpu'
    # fast_dev_run: True
    accelerator: 'gpu'
    devices: 0,1
    strategy:
      class_path: pytorch_lightning.strategies.ddp.DDPStrategy
      init_args:
        find_unused_parameters: False
    check_val_every_n_epoch: 2
    max_epochs: 300
    deterministic: true
  model:
    class_path: comer.modules.CoMERSupervised
    init_args:
      d_model: 256
      # encoder
      growth_rate: 24
      num_layers: 16
      # decoder
      nhead: 8
      num_decoder_layers: 3
      dim_feedforward: 1024
      dropout: 0.3
      dc: 32
      cross_coverage: true
      self_coverage: true
      # beam search
      beam_size: 10
      max_len: 200
      alpha: 1.0
      early_stopping: false
      temperature: 1.0
      # training
      learning_rate: 0.08,
      learning_rate_target: 8e-5,
      steplr_steps: 40
  data:
    class_path: comer.datamodules.CROHMESupvervisedDatamodule
    init_args:
      zipfile_path: "/data/mahlers/synthetic/ntcir12crohmevoc100k.zip"
      test_year: "test"
      val_year: "test"
      train_batch_size: 8
      eval_batch_size: 4
      num_workers: 5
      train_aug: weak
      unlabeled_pct: 0.
      train_sorting: 1
