# IMPORTANT: Extends the base supervised.yaml
# This config file needs to be imported into the Lightning CLI /after/ the base supervised.yaml.
# e.g. train.py -c supervised.yaml -c this.yaml
fit:
  seed_everything: 7
  trainer:
    enable_checkpointing: true
    unlabeled_val_loop: true
    callbacks:
      - class_path: pytorch_lightning.callbacks.LearningRateMonitor
        init_args:
          logging_interval: epoch
      - class_path: pytorch_lightning.callbacks.ModelCheckpoint
        init_args:
          save_top_k: 1
          monitor: val_ExpRate/dataloader_idx_0
          mode: max
          filename: '{epoch}-{step}-{val_ExpRate/dataloader_idx_0:.4f}'
      - class_path: comer.callbacks.SelfTrainingUpdater
        init_args:
          confidence_threshold: 0.6
    accelerator: 'gpu'
    devices: 0,1
    strategy:
      class_path: comer.lit_extensions.DDPUnlabeledStrategy
      init_args:
        find_unused_parameters: False
    check_val_every_n_epoch: 2
    max_epochs: 300
    deterministic: true
    reload_dataloaders_every_n_epochs: 2
  model:
    class_path: comer.modules.CoMERSelfTraining
    init_args:
      d_model: 256
      # encoder
      growth_rate: 24
      num_layers: 16
      # decoder
      nhead: 8
      num_decoder_layers: 3
      dim_feedforward: 1024
      dropout: 0.3
      dc: 32
      cross_coverage: true
      self_coverage: true
      # beam search
      beam_size: 5
      max_len: 200
      alpha: 1.0
      early_stopping: false
      # training
      learning_rate: 0.08,
      learning_rate_target: 8e-5,
      steplr_steps: 8
      # self-training
      pseudo_labeling_threshold: 0.95
  data:
    class_path: comer.datamodules.CROHMESelfTrainingDatamodule
    init_args:
      zipfile_path: data.zip
      test_year: "2019"
      val_year: "2014"
      train_batch_size: 8
      eval_batch_size: 4
      num_workers: 5
      train_aug: weak
